from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import threading
import time

import streamlit as st
import pandas as pd
import numpy as np

# Page configuration
st.set_page_config(
    page_title="My Startup App",
    page_icon="ðŸš€",
    layout="wide"
)

# Header section
st.title("ðŸš€ Welcome to My Startup App")
st.header("ðŸŽ¯ Get Started")
st.write("Ready?")

# Initialize session state variables
if 'counter' not in st.session_state:
    st.session_state.counter = 0
if 'user_data' not in st.session_state:
    st.session_state.user_data = {}
if 'messages' not in st.session_state:
    st.session_state.messages = []

if st.session_state.messages:
    st.write("Message history:")
    for i, msg in enumerate(st.session_state.messages):
        with st.chat_message("user"):
            st.write(msg[0])  # User message
        with st.chat_message("assistant"):
            st.write(msg[1])  # Assistant response


if st.button("ðŸš€ Launch Analysis", type="primary"):
    st.success("Analysis started! Redirecting...")
    st.session_state.messages.append(["hmmm","LOL"])

st.markdown("""
    <style>
    .main > div {
        padding-bottom: 60px;  /* Make space for fixed footer */
    }
    .footer {
        position: fixed;
        left: 0;
        bottom: 0;
        width: 100%;
        background-color: #cccccc;
        padding: 10px 50px;
        display: flex;
        align-items: center;
        box-shadow: 0px -2px 10px rgba(0,0,0,0.1);
        z-index: 1000;
    }
    .footer input {
        flex: 1;
        padding: 10px;
        border: 1px solid #ccc;
        border-radius: 20px;
        margin-right: 10px;
    }
    .footer button {
        background-color: #ff4b4b !important;  /* Red button */
        color: white !important;
        border: none !important;
        border-radius: 20px !important;
        padding: 10px 16px !important;
    }
    .footer button:hover {
        background-color: #e60000 !important;
    }
    </style>
    """, unsafe_allow_html=True)

footer_container = st.container()
with footer_container:
    st.markdown("""<div class='footer'>
        <input type="text" id="userInput" placeholder="Type a message..." />
        <button onclick="">Send</button>
    </div>""", unsafe_allow_html=True)

st.markdown("---")
st.caption("Â© 2024 My Startup App. All rights reserved.")

# Load model and tokenizer

# model_name = "Qwen/Qwen2.5-0.5B-Instruct"
# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     torch_dtype="auto",
#     device_map="auto"
# )
# tokenizer = AutoTokenizer.from_pretrained(model_name)

# # Enable padding if needed
# if not tokenizer.pad_token:
#     tokenizer.pad_token = tokenizer.eos_token

# print("Start chatting with Qwen! (type 'exit' to quit)\n")

# # Conversation history
# messages = [
#     {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."}
# ]

# while True:
#     print("User (type your message, then type END on a new line to send):")
#     lines = []
#     while True:
#         line = input()
#         if line.strip().lower() == "end":
#             break
#         lines.append(line)

#     user_input = "\n".join(lines).strip()

#     if user_input.lower() in ["exit", "quit", "bye"]:
#         print("Exiting chat...")
#         break

#     # Add user message
#     messages.append({"role": "user", "content": user_input})

#     # Apply chat template
#     prompt = tokenizer.apply_chat_template(
#         messages,
#         tokenize=False,
#         add_generation_prompt=True
#     )

#     # Tokenize input
#     inputs = tokenizer([prompt], return_tensors="pt").to(model.device)

#     # Create a streamer for live token generation
#     streamer = TextIteratorStreamer(
#         tokenizer,
#         skip_prompt=True,           # Don't print the prompt again
#         skip_special_tokens=True    # Clean output
#     )

#     # Start generation in a separate thread
#     def generate():
#         model.generate(
#             **inputs,
#             streamer=streamer,
#             max_new_tokens=60000,
#             pad_token_id=tokenizer.eos_token_id,
#             do_sample=True,           # Optional: for more natural responses
#             temperature=0.7,
#         )

#     thread = threading.Thread(target=generate)
#     thread.start()

#     # Print tokens live as they are generated
#     print("Qwen: ", end="", flush=True)
#     response = ""
#     for new_text in streamer:
#         print(new_text, end="", flush=True)
#         response += new_text
#     print("\n")

#     # Add assistant response to history
#     messages.append({"role": "assistant", "content": response})